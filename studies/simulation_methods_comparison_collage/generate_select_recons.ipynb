{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np, torch, scipy.io as io, os, matplotlib.pyplot as plt, h5py, PIL.Image as Image, pathlib\n",
    "import sys\n",
    "sys.path.insert(0, \"../..\")\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "\n",
    "import utils.helper_functions as helper\n",
    "import utils.diffuser_utils as diffuser_utils\n",
    "import dataset.preprocess_data as prep_data\n",
    "import train\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "SAVE_GT_PATH = \"/home/cfoley/defocuscamdata/recons/sim_comparison_figure/model_input_gts\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook to reproduce Figure 6: simulated methods comparison collage\n",
    "\n",
    "We select 4 images from available datasets and visually benchmark the performance of our method against comparable methods, and the ground truth images.\n",
    "All image outputs are false-color projections of 3d hyperspectral volumes\n",
    "\n",
    "### Setup\n",
    "\n",
    "Let's start by getting and standardizing the image samples. You will need to have run the data fetching script in the `/studies` [README.md](../README.md) first, so that this notebook has local access to the sample data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda:0\" # Change this to your desired gpu, or \"cpu\" for CPU processing\n",
    "\n",
    "SAMPLE_DATA_DIRPATH = os.path.join(pathlib.Path().resolve(), \"data\") # TODO: script to get this from google drive. These have some preprocessing applied already\n",
    "OUTPUTS_DIRPATH = os.path.join(pathlib.Path().resolve(), \"outputs\")\n",
    "CONFIGURATION_FILE_DIRPATH = os.path.join(pathlib.Path().resolve(), \"configs\")\n",
    "\n",
    "harvard_bushes = os.path.join(SAMPLE_DATA_DIRPATH, \"imgf8_patch_0.mat\")\n",
    "kaist_img = os.path.join(SAMPLE_DATA_DIRPATH, \"scene03_reflectance.mat\")\n",
    "fruit_artichoke = os.path.join(SAMPLE_DATA_DIRPATH, \"internals_artichoke_SegmentedCroppedCompressed.mat\")\n",
    "icvl_color_checker = os.path.join(SAMPLE_DATA_DIRPATH, \"IDS_COLORCHECK_1020-1223.mat\")\n",
    "\n",
    "icvl_color_checker = prep_data.project_spectral(np.asarray(h5py.File(icvl_color_checker)['rad']).transpose(1,2,0)[::-1, ::-1], 30)[300:820, 200:820]\n",
    "kaist_img = prep_data.project_spectral(io.loadmat(kaist_img)['ref'][300:300+420*5, 200:200+620*5], 30)\n",
    "harvard_bushes = io.loadmat(harvard_bushes)['image']\n",
    "fruit_artichoke = prep_data.project_spectral(prep_data.read_compressed(io.loadmat(fruit_artichoke)), 30).transpose(1,0,2)\n",
    "\n",
    "icvl_color_checker.shape, kaist_img.shape, harvard_bushes.shape, fruit_artichoke.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_image(image, crop_shape, patch_shape):\n",
    "    \"\"\" \n",
    "    Our sample images are all (H X W X C), but of different sizes. \n",
    "    This helper function stantardizes these image shapes, normalizes them, and prepares them to be used as input \n",
    "    to the model.\n",
    "    \"\"\"\n",
    "    image = np.stack(\n",
    "        [diffuser_utils.pyramid_down(image[:crop_shape[0],:crop_shape[1],i],patch_shape) for i in range(image.shape[-1])], 0\n",
    "    )\n",
    "\n",
    "    image = (image - max(0., np.min(image)))\n",
    "    image = image / np.max(image)\n",
    "    image = torch.tensor(image)[None, None,...]\n",
    "    return image\n",
    "\n",
    "\n",
    "def save_image_fc_npy(image, savename, fc_range=(420,720)):\n",
    "    \"\"\" \n",
    "    Helper to save the raw numpy image alongside a false-color projection with a configurable (to the data)\n",
    "    spectral range.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(os.path.dirname(savename)):\n",
    "        os.makedirs(os.path.dirname(savename))\n",
    "\n",
    "    print(\"Saving: \", savename + \".npy\")\n",
    "    np.save(savename + \".npy\", image)\n",
    "\n",
    "    print(\"Saving fc: \", savename + \".png\")\n",
    "    fc_img = helper.select_and_average_bands(image, fc_range=fc_range)\n",
    "    fc_img = Image.fromarray(((fc_img / fc_img.max())*255).astype(np.uint8))\n",
    "    fc_img.save(savename + \".png\")\n",
    "    return fc_img\n",
    "\n",
    "def show_fc(img, fc_range=(420,720)):\n",
    "    \"\"\" \n",
    "    Helper to visualize the false-color projection of an image with a configurable (to the data) spectral\n",
    "    range.\n",
    "    \"\"\"\n",
    "\n",
    "    plt.figure(dpi=100)\n",
    "    rgbimg = helper.select_and_average_bands(img, fc_range=fc_range)\n",
    "    plt.imshow(rgbimg / np.max(rgbimg))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unstandardized images in false color. Let's visualize them here.\n",
    "show_fc(icvl_color_checker)\n",
    "show_fc(fruit_artichoke, fc_range=(400,780))\n",
    "show_fc(harvard_bushes)\n",
    "show_fc(kaist_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next, we standardize the images to a common shape. Let's save these standardized inputs off below, \n",
    "# and visualize them as we do.\n",
    "harvard_bushes_gt = prep_image(harvard_bushes, harvard_bushes.shape[:2], (420,620))\n",
    "fruit_artichoke_gt = prep_image(fruit_artichoke, fruit_artichoke.shape[:2], (420,620))\n",
    "icvl_color_checker_gt = prep_image(icvl_color_checker, icvl_color_checker.shape[:2], (420,620))\n",
    "kaist_img_gt = prep_image(kaist_img, kaist_img.shape[:2], (420,620))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "harvard_bushes_gt_name = os.path.join(OUTPUTS_DIRPATH, \"ground_truth\", \"harvard_bushes\")\n",
    "save_image_fc_npy(harvard_bushes_gt[0,0].numpy().transpose(1,2,0), harvard_bushes_gt_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fruit_artichoke_gt_name = os.path.join(OUTPUTS_DIRPATH, \"ground_truth\", \"fruit_artichoke\")\n",
    "save_image_fc_npy(fruit_artichoke_gt[0,0].numpy().transpose(1,2,0), fruit_artichoke_gt_name, fc_range=(400,780))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "icvl_color_checker_gt_name = os.path.join(OUTPUTS_DIRPATH, \"ground_truth\", \"icvl_color_checker\")\n",
    "save_image_fc_npy(icvl_color_checker_gt[0,0].numpy().transpose(1,2,0), icvl_color_checker_gt_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kaist_img_gt_name = os.path.join(OUTPUTS_DIRPATH, \"ground_truth\", \"kaist_scene03\")\n",
    "save_image_fc_npy(kaist_img_gt[0,0].numpy().transpose(1,2,0), kaist_img_gt_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reconstructions\n",
    "\n",
    "Below, in each section, we run the code to reproduce the reconstructed images in each column of the figure. Since some of these reconstructions run many iterations of FISTA, these cells may take a while. It will help if you have a GPU!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.ensemble import SSLSimulationModel\n",
    "# Some hyperparameters for visualization in this notebook.\n",
    "\n",
    "FISTA_ITERATIONS_BETWEEN_PRINT = 50\n",
    "FISTA_PLOT_WITH_PRINT = False # set to True if you want to see intermediate FISTA reconstructions\n",
    "\n",
    "handshake_fista_config = os.path.join(CONFIGURATION_FILE_DIRPATH, \"handshake_fista.yml\")\n",
    "diffusercam_fista_config = os.path.join(CONFIGURATION_FILE_DIRPATH, \"diffusercam_fista.yml\")\n",
    "defocuscam_fista_config = os.path.join(CONFIGURATION_FILE_DIRPATH, \"defocuscam_fista.yml\")\n",
    "defocuscam_learned_config = os.path.join(CONFIGURATION_FILE_DIRPATH, \"defocuscam_learned.yml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(config_path) -> SSLSimulationModel:\n",
    "    \"\"\"\n",
    "    Get a SSLSimulationModel instance configured with a forward model that simulates our\n",
    "    hyperspectral imager of choice (as per the config file), and a learned or iterative\n",
    "    reconstruction model to solve the inverse problem.\n",
    "    \"\"\"\n",
    "    config = helper.read_config(config_path)\n",
    "    model = train.get_model(config, device=device)\n",
    "\n",
    "    print(\"Simulation model loaded: \", model.model1.operations)\n",
    "    print(f\"Reconstruction model loaded: {model.model2}\")\n",
    "    return model\n",
    "\n",
    "def get_fista_model(config_path: str, fista_lr_mult = 1.0) -> SSLSimulationModel:\n",
    "    \"\"\" Boilerplate additional steps when loading a model with FISTA recons\"\"\"\n",
    "    model = get_model(config_path)\n",
    "    rm = model.model2\n",
    "\n",
    "    rm.L = rm.L * fista_lr_mult\n",
    "    rm.print_every = FISTA_ITERATIONS_BETWEEN_PRINT\n",
    "    rm.plot = FISTA_PLOT_WITH_PRINT\n",
    "    print(f\"FISTA params: prior={rm.prox_method}, L={rm.L}, tau={rm.tau}, tv_lambda=\"\n",
    "          f\"{rm.tv_lambda}, tv_lambdaw={rm.tv_lambdaw}, tv_lambdax={rm.tv_lambdax}\")\n",
    "    return model\n",
    "\n",
    "def run_fista_single_image(model, image):\n",
    "    \"\"\"\" Run the hyperspectral image through our simulation and reconstruction models.\"\"\"\n",
    "    forward_model, recon_model = model.model1, model.model2\n",
    "    simulated_measurement = forward_model(image.to(recon_model.device))\n",
    "    recon_model(simulated_measurement.squeeze(dim=(0,2)).to(recon_model.device))\n",
    "    return recon_model.out_img\n",
    "\n",
    "def save_reconstruction(recon: np.ndarray, save_namekey: str, fc_range=(420,720)):\n",
    "    \"\"\" Boilerplate for saving off the fist model reconstructions with interpretable names.\"\"\"\n",
    "    out_path_stem = os.path.join(OUTPUTS_DIRPATH,  \"reconstructions\",  save_namekey)\n",
    "    saved_fc_image = save_image_fc_npy(recon, out_path_stem, fc_range)\n",
    "    return saved_fc_image  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### HSI DiffuserCam with FISTA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diffusercam_fista_model = get_fista_model(diffusercam_fista_config, fista_lr_mult=0.2)\n",
    "\n",
    "harvard_bushes_diffusercam_fista_recon = run_fista_single_image(diffusercam_fista_model, harvard_bushes_gt)\n",
    "display(save_reconstruction(harvard_bushes_diffusercam_fista_recon, \"harvard_bushes_diffusercam_fista_recon\"))\n",
    "\n",
    "icvl_color_checker_diffusercam_fista_recon = run_fista_single_image(diffusercam_fista_model, icvl_color_checker_gt)\n",
    "display(save_reconstruction(icvl_color_checker_diffusercam_fista_recon, \"icvl_color_checker_diffusercam_fista_recon\"))\n",
    "\n",
    "fruit_artichoke_diffusercam_fista_recon = run_fista_single_image(diffusercam_fista_model, fruit_artichoke_gt)\n",
    "display(save_reconstruction(fruit_artichoke_diffusercam_fista_recon, \"fruit_artichoke_diffusercam_fista_recon\"))\n",
    "\n",
    "kaist_scene03_diffusercam_fista_recon = run_fista_single_image(diffusercam_fista_model, kaist_img_gt)\n",
    "display(save_reconstruction(kaist_scene03_diffusercam_fista_recon, \"kaist_scene03_diffusercam_fista_recon\"))\n",
    "\n",
    "del diffusercam_fista_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Handshake Camera with FISTA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "handshake_fista_model = get_fista_model(handshake_fista_config, fista_lr_mult=0.1)\n",
    "\n",
    "harvard_bushes_handshake_fista_recon = run_fista_single_image(handshake_fista_model, harvard_bushes_gt)\n",
    "display(save_reconstruction(harvard_bushes_handshake_fista_recon, \"harvard_bushes_handshake_fista_recon\"))\n",
    "\n",
    "icvl_color_checker_handshake_fista_recon = run_fista_single_image(handshake_fista_model, icvl_color_checker_gt)\n",
    "display(save_reconstruction(icvl_color_checker_handshake_fista_recon, \"icvl_color_checker_handshake_fista_recon\"))\n",
    "\n",
    "fruit_artichoke_handshake_fista_recon = run_fista_single_image(handshake_fista_model, fruit_artichoke_gt)\n",
    "display(save_reconstruction(fruit_artichoke_handshake_fista_recon, \"fruit_artichoke_handshake_fista_recon\"))\n",
    "\n",
    "kaist_scene03_handshake_fista_recon = run_fista_single_image(handshake_fista_model, kaist_img_gt)\n",
    "display(save_reconstruction(kaist_scene03_handshake_fista_recon, \"kaist_scene03_handshake_fista_recon\"))\n",
    "\n",
    "del handshake_fista_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DefocusCam with FISTA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "defocuscam_fista_model = get_fista_model(defocuscam_fista_config, fista_lr_mult=1)\n",
    "\n",
    "harvard_bushes_defocuscam_fista_recon = run_fista_single_image(defocuscam_fista_model, harvard_bushes_gt)\n",
    "display(save_reconstruction(harvard_bushes_defocuscam_fista_recon, \"harvard_bushes_defocuscam_fista_recon\"))\n",
    "\n",
    "icvl_color_checker_defocuscam_fista_recon = run_fista_single_image(defocuscam_fista_model, icvl_color_checker_gt)\n",
    "display(save_reconstruction(icvl_color_checker_defocuscam_fista_recon, \"icvl_color_checker_defocuscam_fista_recon\"))\n",
    "\n",
    "fruit_artichoke_defocuscam_fista_recon = run_fista_single_image(defocuscam_fista_model, fruit_artichoke_gt)\n",
    "display(save_reconstruction(fruit_artichoke_defocuscam_fista_recon, \"fruit_artichoke_defocuscam_fista_recon\"))\n",
    "\n",
    "kaist_scene03_defocuscam_fista_recon = run_fista_single_image(defocuscam_fista_model, kaist_img_gt)\n",
    "display(save_reconstruction(kaist_scene03_defocuscam_fista_recon, \"kaist_scene03_defocuscam_fista_recon\"))\n",
    "\n",
    "del defocuscam_fista_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DefocusCam with a learned CondUNet\n",
    "\n",
    "Since our learned model's weights are learned on a square patch of the spectral filter mask, with square psfs,\n",
    "we need to predict in patches, and blend the results together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from patch_predict_utils import patchwise_predict_image_learned\n",
    "\n",
    "defocuscam_learned_model = get_model(defocuscam_learned_config)\n",
    "\n",
    "harvard_bushes_defocuscam_learned_recon = patchwise_predict_image_learned(defocuscam_learned_model, harvard_bushes_gt)\n",
    "display(save_reconstruction(harvard_bushes_defocuscam_learned_recon, \"harvard_bushes_defocuscam_learned_recon\"))\n",
    "\n",
    "icvl_color_checker_defocuscam_learned_recon = patchwise_predict_image_learned(defocuscam_learned_model, icvl_color_checker_gt)\n",
    "display(save_reconstruction(icvl_color_checker_defocuscam_learned_recon, \"icvl_color_checker_defocuscam_learned_recon\"))\n",
    "\n",
    "fruit_artichoke_defocuscam_learned_recon = patchwise_predict_image_learned(defocuscam_learned_model, fruit_artichoke_gt)\n",
    "display(save_reconstruction(fruit_artichoke_defocuscam_learned_recon, \"fruit_artichoke_defocuscam_learned_recon\"))\n",
    "\n",
    "kaist_scene03_defocuscam_learned_recon = patchwise_predict_image_learned(defocuscam_learned_model, kaist_img_gt)\n",
    "display(save_reconstruction(kaist_scene03_defocuscam_learned_recon, \"kaist_scene03_defocuscam_learned_recon\"))\n",
    "\n",
    "del defocuscam_learned_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchjax_310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
