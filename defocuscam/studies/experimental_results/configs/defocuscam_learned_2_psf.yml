# ------------------------------------------ #
# -------------- DO NOT CHANGE ------------- #
# -- STATIC CONFIG FOR EXPERIMENTAL MODEL -- #
# ------------------------------------------ #

# data paths
checkpoint_dir: "../../../data/models/checkpoint_results_learned_largecrop_config.yml/2024_03_16_21_09_51/saved_model_ep49_testloss_0.05782177185882693.pt"
psf_dir: "../../../data/calibration_data/lsi_psfs_raw"
base_data_path: "../../../data/sample_data_preprocessed/sample_data_preprocessed_largecrop_shotnoise_lsi_02_07_L1psf_2meas"
mask_dir: "../../../data/calibration_data/calibration_matrix_390-870_30chan_stride16_avg16.mat"

# model params
device: 2
data_precomputed: True
passthrough: True
forward_model_params:
  stack_depth: 2
  psf:
    lri: False
    stride: 4
    symmetric: True
    optimize: False
    padded_shape: [1260, 1860] # only used if LRI is False
    largest_psf_diam: 128
    exposures: [0.00151, 0.04761] # exposures of eahc psf level
    threshold: 0.55 # noise threshold
    norm: one # applies normalization to each psf ("one": L1, "two": L2)
  operations:
    sim_blur: False
    sim_meas: True
    adjoint: False
    spectral_pad: False
    adj_mask_noise: False
    fwd_mask_noise: True
    shot_noise: True
  mask_noise: # augment mask with noise
    intensity: [0, 0.01]
    stopband_only: true
    type: gaussian
  shot_noise:
    intensity: [0, 0.01]

recon_model_params:
  model_name: "unet_conditioned"
  norm: "local_response"
  
# training params
epochs: 100
batch_size: 2
grad_accumulate: 2
patch_size: [420, 620] # inference & training size (NOTE: downsampling reduces filter precision)
patch_crop: [1260, 1860] # actual size on camera
image_center: [1200, 1970] # center of both the image and mask crops
num_workers: 6
data_partition: [0.7, 0.15, 0.15] #["fruit", "pavia", "harvard"] # train, val, test
early_stopping_patience: 20
loss_function: 
  name: "mse" # mse, mae, cossim, lpips, psnr, ssim
  params: 
optimizer: 
  name: "adam" # adam, sgd, adagrad, rmsprop
  params: # kwargs
    lr: 0.0001
    weight_decay: 0.001
lr_scheduler: 
  name: "exponential" # warm_cosine_anneal, exponential, plateau
  params: 
    gamma: 0.94

# restarting from checkpoint
preload_weights: True
offset: 0

# other params
no_graph: True
log_grads: False
validation_stride: 1
validation_iter_stride: 100
checkpoint_stride: 1