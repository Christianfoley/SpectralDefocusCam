import torch
import random
import scipy.io as io
import os
import glob
import time

from torch.utils.data import Dataset
from torchvision import transforms
from torch.utils.data import DataLoader


def get_data_precomputed(batch_size, data_split, base_path, workers=1, model_params={}):
    """
    Return dataloaders for the train, test, and val partitions of a dataset. Dataset should be
    "pre-computed", as this dataset will directly return the raw data, with no augmentations.

    Samples should be stored with their ground-truth pairs in the same .mat files. Ground truths
    should be stored under the key 'image', and precomputed data should be stored under a key
    generated by the params of the forward model.

    Parameters
    ----------
    batch_size : int
        Size of the loading batch.
    data_split : tuple
        A tuple containing the proportions for the train, validation, and test sets.
        These proportions must sum to 1.
    base_path : str
        Path to the top directory containing precomputed data in .mat files.
    workers : int, optional
        Number of dataloading workers. Default is 1.
    model_params : dict, optional
        Parameters for the forward model used to generate keys for precomputed data.
        Default is an empty dictionary.

    Returns
    -------
    tuple
        tuple of dataloaders (train, val, test)
    """
    # define transform and partition data
    train_files, val_files, test_files = partition(*data_split, base_path)
    transform = transforms.Compose([toTensor(), Normalize(0, 1)])
    target_transform = transforms.Compose([toTensor(), ReArrange(), Normalize(0, 1)])

    # make datasets
    train = PrecomputedDataset(train_files, transform, target_transform, model_params)
    val = PrecomputedDataset(val_files, transform, target_transform, model_params)
    test = PrecomputedDataset(test_files, transform, target_transform, model_params)

    # make dataloaders for pytorch
    train_dataloader = DataLoader(train, batch_size, shuffle=True, num_workers=workers)
    val_dataloader = DataLoader(val, batch_size, shuffle=True, num_workers=workers)
    test_dataloader = DataLoader(test, batch_size)

    return train_dataloader, val_dataloader, test_dataloader


def partition(train, val, test, base_path):
    """
    Partition data in the provided paths (at random) into train, test, and validation sets
    depending on the proportions specified in train, test, and val. Note that train, test,
    and val must sum to 1.

    If base_path is a directory which contains directories, partition will recursively search
    for .mat files in the subdirectories.

    Parameters
    ----------
    train : float or str
        proportion (of 1) of dat in train set
    val : float or str
        proportion (of 1) of dat in test set
    test : float or str
        proportion (of 1) of dat in validation set
    base_path : str
        path to top directory containing .mat sample files

    Returns
    -------
    tuple(list, list, list)
        list of filepaths to samples in the train, test, and validation sets
    """
    if isinstance(train, str):
        possible = {"harvard", "fruit", "pavia"}
        assert train in possible and val in possible and test in possible
    elif train + val + test != 1.0:
        raise ValueError("The sum of train, val, and test should be 1")

    # Function to get all .mat file paths within a directory and its subdirectories
    def get_mat_files(directory):
        mat_files = []
        for dirpath, _, filenames in os.walk(directory):
            for filename in filenames:
                if filename.endswith(".mat"):
                    mat_files.append(os.path.join(dirpath, filename))
        return mat_files

    all_files = get_mat_files(base_path)
    random.shuffle(all_files)

    if isinstance(train, str):
        # In shuffled order separate out by source dataset
        train_set = [file for file in all_files if train in file]
        val_set = [file for file in all_files if val in file]
        test_set = [file for file in all_files if test in file]
    else:
        # Calculate the number of files for each set
        num_files = len(all_files)
        num_train = int(train * num_files)
        num_val = int(val * num_files)

        # Divide the shuffled files into train, test, and validation sets
        train_set = all_files[:num_train]
        val_set = all_files[num_train : num_train + num_val]
        test_set = all_files[num_train + num_val :]

    return train_set, val_set, test_set


class PrecomputedDataset(Dataset):
    """
    Custom dataset class that loads precomputed data from .npy files and allows transformation.

    Parameters
    ----------
    file_list : list
        List of file paths containing precomputed data in .npy format.
    transform : function, optional
        A function to apply transformations to the loaded input data. Default is None.
    target_transform : function
        A function to apply transformations to the loaded target data. Default is None
    model_params : dict, optional
        Parameters for the forward model used to generate keys for precomputed data.


    Attributes
    ----------
    file_list : list
        List of file paths containing precomputed data.
    transform : function
        A function to apply transformations to the loaded input data.
    target_transform : function
        A function to apply transformations to the loaded target data.
    readtime : float
        Cumulative time taken to read data.

    Methods
    -------
    __len__()
        Returns the total number of samples in the dataset.
    __getitem__(idx)
        Loads and returns the data sample at the given index, applies transformations if specified.

    Raises
    ------
    IndexError
        If an error occurs during transformation or loading of data for a specific index.

    Notes
    -----
    The dataset assumes that the provided file_list contains paths to precomputed data in .mat format.
    """

    def __init__(
        self, file_list, transform=None, target_transform=None, model_params={}
    ):
        self.file_list = file_list
        self.transform = transform
        self.target_transform = target_transform
        self.model_params = model_params
        self.readtime = 0

    def __len__(self):
        """
        Returns the total number of samples in the dataset.

        Returns
        -------
        int
            Total number of samples in the dataset.
        """
        return len(self.file_list)

    def __getitem__(self, idx):
        """
        Loads and returns the data sample at the given index, applies transformations if specified.

        Parameters
        ----------
        idx : int
            Index of the sample to retrieve.

        Returns
        -------
        dict
            A dictionary containing the loaded data sample, labeled as 'image'.

        Raises
        ------
        IndexError
            If an error occurs during transformation or loading of data for a specific index.
        """
        start = time.time()
        data = io.loadmat(self.file_list[idx])
        key = self.get_data_key(data.keys())
        y, x = data["image"], data[key]

        if len(x.shape) == 5:
            assert x.shape[0] == 1, f"Found batched input:  {self.file_list[idx]}"
            x = x[0]

        if self.transform:
            try:
                x = self.transform(x)
            except Exception as e:
                raise IndexError(
                    f"Error with index {idx}, {e} \n shape: {x.shape} \n filename: {self.file_list[idx]}"
                )

        if self.target_transform:
            try:
                y = self.target_transform(y)
            except Exception as e:
                raise IndexError(
                    f"Error with index {idx}, {e} \n shape: {y.shape} \n filename: {self.file_list[idx]}"
                )

        self.readtime += time.time() - start
        return {"image": y, "input": x}

    def get_data_key(self, key_list):
        """
        Gets the corresponding parameter key from the keylist

        Parameters
        ----------
        key_list : list
            list of keys in loaded .mat file

        Returns
        -------
        str
            correct key corresponding to the param dict key of the object
        """
        key = None
        for key in key_list:
            try:
                param_dict = eval(key)
                matching_dict = param_dict.copy()
                if "timestamp" in matching_dict:
                    matching_dict.pop("timestamp")
                if matching_dict.items() == self.model_params.items():
                    key = str(param_dict)
                    break
            except:
                continue
        assert key is not None, "No key has been found"
        return key


class toTensor(object):
    """
    Transform to convert input to a PyTorch tensor and  enforces the output tensor to be of dtype
    torch.float32.
    If `device` is provided, the resulting tensor will be placed on the specified device.

    Parameters
    ----------
    sample : object
        Input data to be converted to a PyTorch tensor.
    device : torch.device, optional
        Device specification for the tensor. Default is None.

    Returns
    -------
    torch.Tensor
        A PyTorch tensor converted from the input data, with a dtype of torch.float32.
    """

    def __call__(self, sample, device=None):
        """
        Convert the input sample to a PyTorch tensor with a dtype of torch.float32.

        Parameters
        ----------
        sample : object
            Input data to be converted to a PyTorch tensor.
        device : torch.device, optional
            Device specification for the tensor. Default is None.

        Returns
        -------
        torch.Tensor
            A PyTorch tensor converted from the input data, with a dtype of torch.float32.
        """
        if device:
            sample = torch.tensor(sample, dtype=torch.float32, device=device)
        else:
            sample = torch.tensor(sample, dtype=torch.float32)
        return sample


class Normalize(object):
    """
    Transform to normalize a tensor to a specified mean and std. If specified,
    will prevent normalization along specified axes by index.

    Parameters
    ----------
    mean : float
        Specified mean value of output tensor, by default 0.
    std : float
        Specified std value of output tensor, by default 1.
    ignore_dims : tuple(int)
        The axes to ignore during normalization.

    Notes
    -----
    The `ignored_axis` parameter specifies the axis that will not be normalized.
    """

    def __init__(self, mean=0, std=1, ignore_dims=()):
        self.mean = mean
        self.std = std
        self.ignore_dims = ignore_dims

    def __call__(self, tensor: torch.Tensor):
        """
        Apply normalization to the sample tensor.

        Parameters
        ----------
        tensor : torch.Tensor
            Input tensor to be normalized.

        Returns
        -------
        torch.Tensor
            Normalized sample tensor.
        """
        mean = tensor.mean(dim=self.ignore_dims, keepdim=True)
        std = tensor.std(dim=self.ignore_dims, unbiased=False, keepdim=True)
        std[std == 0] = 1  # Handling potential division by zero
        return ((tensor - mean + self.mean) / std) * self.std


class ReArrange(object):
    """
    Transform to rearrange a tensor from shape (x, y, c) to (c, y, x).
    """

    def __call__(self, tensor: torch.Tensor):
        """
        Rearrange the input tensor from (x, y, c) to (c, y, x).

        Parameters
        ----------
        tensor : torch.Tensor
            Input tensor to be rearranged.

        Returns
        -------
        torch.Tensor
            Rearranged tensor in the shape (c, y, x).
        """
        if tensor.dim() > 3:
            tensor = tensor.squeeze()  # Remove the extra dimensions

        return tensor.permute(2, 0, 1)
