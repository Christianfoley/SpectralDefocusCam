import torch
import numpy as np
import random
import scipy.io as io
import os
import glob
import time

from torch.utils.data import Dataset
from torchvision import transforms
from torch.utils.data import DataLoader

# SETS RANDOM SEED DO NOT CHANGE
SEED = 6.626
random.seed(SEED)
np.random.seed(int(SEED))
torch.manual_seed(SEED)


def get_data_precomputed(
    batch_size,
    data_split,
    base_path,
    workers=1,
    model_params={},
    shuffle=True,
    norm_target=True,
):
    """
    Return dataloaders for the train, test, and val partitions of a dataset. Dataset should be
    "pre-computed", as this dataset will directly return the raw data, with no augmentations.

    Samples should be stored with their ground-truth pairs in the same .mat files. Ground truths
    should be stored under the key 'image', and precomputed data should be stored under a key
    generated by the params of the forward model.

    Parameters
    ----------
    batch_size : int
        Size of the loading batch.
    data_split : tuple
        A tuple containing the proportions for the train, validation, and test sets.
        These proportions must sum to 1.
    base_path : str
        Path to the top directory containing precomputed data in .mat files.
    workers : int, optional
        Number of dataloading workers. Default is 1.
    model_params : dict, optional
        Parameters for the forward model used to generate keys for precomputed data.
        Default is an empty dictionary.
    shuffle : bool, optional
        Whether to shuffle samples in dataloaders, by default True.
    norm_target : bool, optional
        Whether to normalize target, important for output numerical range, by default true

    Returns
    -------
    tuple
        tuple of dataloaders (train, val, test)
    """
    # define transform and partition data
    train_files, val_files, test_files = partition(*data_split, base_path)
    transform = transforms.Compose([toTensor(), Normalize(0, 1)])
    target_transform = transforms.Compose(
        [toTensor(), ReArrange(), Normalize(0, 1)]
        if norm_target
        else [toTensor(), ReArrange()]
    )

    # make datasets
    train = PrecomputedDataset(train_files, transform, target_transform, model_params)
    val = PrecomputedDataset(val_files, transform, target_transform, model_params)
    test = PrecomputedDataset(test_files, transform, target_transform, model_params)

    # make dataloaders for pytorch
    train_dataloader = DataLoader(train, batch_size, shuffle, num_workers=workers)
    val_dataloader = DataLoader(val, batch_size, shuffle, num_workers=workers)
    test_dataloader = DataLoader(test, batch_size)

    return train_dataloader, val_dataloader, test_dataloader


def partition(train, val, test, base_path):
    """
    Partition data in the provided paths (at random) into train, test, and validation sets
    depending on the proportions specified in train, test, and val. Note that train, test,
    and val must sum to 1.

    If base_path is a directory which contains directories, partition will recursively search
    for .mat files in the subdirectories.

    Parameters
    ----------
    train : float or str
        proportion (of 1) of imgs in train set, or unique keyword of filename
    val : float or str
        proportion (of 1) of imgs in test set, or unique keyword of filename
    test : float or str
        proportion (of 1) of imgs in validation set, or unique keyword of filename
    base_path : str
        path to top directory containing .mat sample files

    Returns
    -------
    tuple(list, list, list)
        list of filepaths to samples in the train, test, and validation sets
    """
    if isinstance(train, str):
        possible = {"harvard", "fruit", "pavia"}
        assert train in possible and val in possible and test in possible
    elif train + val + test != 1.0:
        raise ValueError("The sum of train, val, and test should be 1")

    # Get all .mat file paths within a directory and its subdirectories
    def get_mat_files(directory):
        mat_files = []
        for dirpath, _, filenames in os.walk(directory):
            for filename in filenames:
                if filename.endswith(".mat"):
                    mat_files.append(os.path.join(dirpath, filename))
        return sorted(mat_files)

    all_files = get_mat_files(base_path)

    if isinstance(train, str):
        # In shuffled order separate out by source dataset
        random.shuffle(all_files)
        train_set = [file for file in all_files if train in file]
        val_set = [file for file in all_files if val in file]
        test_set = [file for file in all_files if test in file]
    else:
        # To prevent leakage, separate patches by their source image
        patches_by_src = {}
        for f in all_files:
            src = os.path.basename(f).split("_patch_")[0]
            if src in patches_by_src:
                patches_by_src[src].append(f)
            else:
                patches_by_src[src] = [f]
        randomkeys = list(patches_by_src.keys())
        random.shuffle(randomkeys)

        # Partition source images
        num_files = len(randomkeys)
        n_train, n_val = int(train * num_files), int(val * num_files)

        train_src = randomkeys[:n_train]
        val_src = randomkeys[n_train : n_train + n_val]
        test_src = randomkeys[n_train + n_val :]

        # Pool together patches from source images and shuffle
        data = [[], [], []]
        for i, src in enumerate([train_src, val_src, test_src]):
            for key in src:
                for file in patches_by_src[key]:
                    data[i].append(file)
            random.shuffle(data[i])
        train_set, val_set, test_set = data

    return train_set, val_set, test_set


class PrecomputedDataset(Dataset):
    """
    Custom dataset class that loads precomputed data from .npy files and allows transformation.

    Parameters
    ----------
    file_list : list
        List of file paths containing precomputed data in .npy format.
    transform : function, optional
        A function to apply transformations to the loaded input data. Default is None.
    target_transform : function
        A function to apply transformations to the loaded target data. Default is None
    model_params : dict, optional
        Parameters for the forward model used to generate keys for precomputed data.


    Attributes
    ----------
    file_list : list
        List of file paths containing precomputed data.
    transform : function
        A function to apply transformations to the loaded input data.
    target_transform : function
        A function to apply transformations to the loaded target data.
    readtime : float
        Cumulative time taken to read data.

    Methods
    -------
    __len__()
        Returns the total number of samples in the dataset.
    __getitem__(idx)
        Loads and returns the data sample at the given index, applies transformations if specified.

    Raises
    ------
    IndexError
        If an error occurs during transformation or loading of data for a specific index.

    Notes
    -----
    The dataset assumes that the provided file_list contains paths to precomputed data in .mat format.
    """

    def __init__(
        self, file_list, transform=None, target_transform=None, model_params={}
    ):
        self.file_list = file_list
        self.transform = transform
        self.target_transform = target_transform
        self.model_params = model_params
        self.readtime = 0

    def __len__(self):
        """
        Returns the total number of samples in the dataset.

        Returns
        -------
        int
            Total number of samples in the dataset.
        """
        return len(self.file_list)

    def __getitem__(self, idx):
        """
        Loads and returns the data sample at the given index, applies transformations if specified.

        Parameters
        ----------
        idx : int
            Index of the sample to retrieve.

        Returns
        -------
        dict
            A dictionary containing the loaded data sample, labeled as 'image'.

        Raises
        ------
        IndexError
            If an error occurs during transformation or loading of data for a specific index.
        """
        start = time.time()
        data = io.loadmat(self.file_list[idx])
        key = self.get_data_key(data.keys())
        y, x = data["image"], data[key]

        # handling various use cases - need to force into (nlyx) tensor format
        if len(x.shape) == 3:
            x = x.transpose(2, 0, 1)[None, ...]
        if len(x.shape) == 5:
            assert x.shape[0] == 1, f"Found batched input:  {self.file_list[idx]}"
            x = x[0]

        if self.transform:
            try:
                x = self.transform(x)
            except Exception as e:
                raise IndexError(
                    f"Error with index {idx}, {e} \n shape: {x.shape} \n filename: {self.file_list[idx]}"
                )

        if self.target_transform:
            try:
                y = self.target_transform(y)
            except Exception as e:
                raise IndexError(
                    f"Error with index {idx}, {e} \n shape: {y.shape} \n filename: {self.file_list[idx]}"
                )

        self.readtime += time.time() - start
        return {"image": y, "input": x}

    def get_data_key(self, key_list):
        """
        Gets the corresponding parameter key from the keylist.
        As a verification step, attempts to match current model parameters to the creation model of
        the given data. If no key is found, will assume that the longest key corresponds to the
        parameter dictionary, and proceed with a warning.

        Parameters
        ----------
        key_list : list
            list of keys in loaded .mat file

        Returns
        -------
        str
            correct key corresponding to the param dict key of the object
        """
        key = None
        for key in key_list:
            try:
                param_dict = eval(key)
                matching_dict = param_dict.copy()
                if "timestamp" in matching_dict:
                    matching_dict.pop("timestamp")
                if matching_dict.items() == self.model_params.items():
                    key = str(param_dict)
                    break
            except:
                continue
        if key is None:
            print(
                "Warning: No matching data key has been found. This may not be the correct dataset. "
                "Proceeding with key = 'image'."
            )
            key = "image"
        return key


class toTensor(object):
    """
    Transform to convert input to a PyTorch tensor and  enforces the output tensor to be of dtype
    torch.float32.
    If `device` is provided, the resulting tensor will be placed on the specified device.

    Parameters
    ----------
    sample : object
        Input data to be converted to a PyTorch tensor.
    device : torch.device, optional
        Device specification for the tensor. Default is None.

    Returns
    -------
    torch.Tensor
        A PyTorch tensor converted from the input data, with a dtype of torch.float32.
    """

    def __call__(self, sample, device=None):
        """
        Convert the input sample to a PyTorch tensor with a dtype of torch.float32.

        Parameters
        ----------
        sample : object
            Input data to be converted to a PyTorch tensor.
        device : torch.device, optional
            Device specification for the tensor. Default is None.

        Returns
        -------
        torch.Tensor
            A PyTorch tensor converted from the input data, with a dtype of torch.float32.
        """
        if device:
            sample = torch.tensor(sample, dtype=torch.float32, device=device)
        else:
            sample = torch.tensor(sample, dtype=torch.float32)
        return sample


class Normalize(object):
    """
    Transform to normalize a tensor to a specified mean and std. If specified,
    will prevent normalization along specified axes by index.

    Parameters
    ----------
    mean : float
        Specified mean value of output tensor, by default 0.
    std : float
        Specified std value of output tensor, by default 1.
    ignore_dims : tuple(int)
        The axes to ignore during normalization.

    Notes
    -----
    The `ignored_axis` parameter specifies the axis that will not be normalized.
    """

    def __init__(self, mean=0, std=1, ignore_dims=()):
        self.mean = mean
        self.std = std
        self.ignore_dims = ignore_dims

    def __call__(self, tensor: torch.Tensor):
        """
        Apply normalization to the sample tensor.

        Parameters
        ----------
        tensor : torch.Tensor
            Input tensor to be normalized.

        Returns
        -------
        torch.Tensor
            Normalized sample tensor.
        """
        mean = tensor.mean(dim=self.ignore_dims, keepdim=True)
        std = tensor.std(dim=self.ignore_dims, unbiased=False, keepdim=True)
        std[std == 0] = 1  # Handling potential division by zero
        return ((tensor - mean) / std) * self.std + self.mean


class ReArrange(object):
    """
    Transform to rearrange a tensor from shape (x, y, c) to (c, y, x).
    """

    def __call__(self, tensor: torch.Tensor):
        """
        Rearrange the input tensor from (x, y, c) to (c, y, x).

        Parameters
        ----------
        tensor : torch.Tensor
            Input tensor to be rearranged.

        Returns
        -------
        torch.Tensor
            Rearranged tensor in the shape (c, y, x).
        """
        if tensor.dim() > 3:
            tensor = tensor.squeeze()  # Remove the extra dimensions

        return tensor.permute(2, 0, 1)
